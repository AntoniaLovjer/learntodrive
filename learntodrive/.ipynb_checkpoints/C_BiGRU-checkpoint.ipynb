{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "papermill": {
     "duration": 0.028295,
     "end_time": "2019-10-20T00:04:03.343136",
     "exception": false,
     "start_time": "2019-10-20T00:04:03.314841",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "papermill": {
     "duration": 0.670129,
     "end_time": "2019-10-20T00:04:04.024039",
     "exception": false,
     "start_time": "2019-10-20T00:04:03.353910",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from learntodrive.utils import move_target_to_cuda\n",
    "from learntodrive.utils import move_data_to_cuda\n",
    "from learntodrive.utils import log_textfile\n",
    "from learntodrive.utils import loadPickle\n",
    "from learntodrive.utils import get_features_3\n",
    "\n",
    "from learntodrive.submission import predict_submission\n",
    "from learntodrive.submission import create_submission\n",
    "\n",
    "from learntodrive.dataloader_buckets import Drive360Loader\n",
    "\n",
    "from learntodrive.validation import run_validation\n",
    "\n",
    "from learntodrive.models.BiGRU import BiGRU\n",
    "from learntodrive.models.BiGRU import BiGRU_DO\n",
    "\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import time\n",
    "from random import shuffle\n",
    "import numpy as np\n",
    "import json\n",
    "import sys"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "w0Z-D3MMaWAr",
    "papermill": {
     "duration": 0.010188,
     "end_time": "2019-10-20T00:04:04.044965",
     "exception": false,
     "start_time": "2019-10-20T00:04:04.034777",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### Configuration file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ztA2jxchjizf",
    "papermill": {
     "duration": 0.027808,
     "end_time": "2019-10-20T00:04:04.083827",
     "exception": false,
     "start_time": "2019-10-20T00:04:04.056019",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "config = json.load(open('config_sample1.json'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "papermill": {
     "duration": 0.036004,
     "end_time": "2019-10-20T00:04:04.130270",
     "exception": false,
     "start_time": "2019-10-20T00:04:04.094266",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'cuda': {'use': True},\n",
       " 'front': True,\n",
       " 'multi_camera': {'right_left': False, 'rear': False},\n",
       " 'data_loader': {'historic': {'number': 10, 'frequency': 1},\n",
       "  'data_dir': './Data/Sample1/',\n",
       "  'train': {'csv_name': 'train_sample1.csv',\n",
       "   'batch_size': 2,\n",
       "   'shuffle': True,\n",
       "   'num_workers': 8},\n",
       "  'validation': {'csv_name': 'val_sample1.csv',\n",
       "   'batch_size': 2,\n",
       "   'shuffle': True,\n",
       "   'num_workers': 8},\n",
       "  'test': {'csv_name': 'test_sample1.csv',\n",
       "   'batch_size': 2,\n",
       "   'shuffle': False,\n",
       "   'num_workers': 1}},\n",
       " 'target': {'normalize': True,\n",
       "  'mean': {'canSteering': -5.406788214535221, 'canSpeed': 13.426163367846936},\n",
       "  'std': {'canSteering': 73.41232589456718, 'canSpeed': 7.8257638553586455}},\n",
       " 'image': {'norm': {'mean': [0.4443069311879691,\n",
       "    0.44355877047930287,\n",
       "    0.4447293861201888],\n",
       "   'std': [0.08480363653014882, 0.08435648892210044, 0.08600841133226468]}}}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "BiQeSNuDjjW2",
    "papermill": {
     "duration": 0.033326,
     "end_time": "2019-10-20T00:04:04.174689",
     "exception": false,
     "start_time": "2019-10-20T00:04:04.141363",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# config['data_loader']['historic']['frequency'] = 1\n",
    "# config['data_loader']['historic']['number'] = 10\n",
    "config['data_loader']['train']['batch_size'] = 65\n",
    "config['data_loader']['validation']['batch_size'] = 65\n",
    "config['data_loader']['test']['batch_size'] = 65\n",
    "\n",
    "densenet121_aug = '../Pickle/sample1_extract_densen121_epoch90_aug_own_features_v3_v2.pickle'\n",
    "resnet34_high = '../Pickle/sample1_resnet34_high_features_v3_v2.pickle'\n",
    "densenet201_high = '../Pickle/sample1_densenet201_features_v3_v2.pickle'\n",
    "dict_pickle = {\n",
    "    'densenet121_aug': densenet121_aug,\n",
    "    'resnet34_high': resnet34_high,\n",
    "    'densenet201_high': densenet201_high\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "bD3xbHMNd0wq",
    "papermill": {
     "duration": 0.010635,
     "end_time": "2019-10-20T00:04:04.235049",
     "exception": false,
     "start_time": "2019-10-20T00:04:04.224414",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### A simple driving model training and evaluation pipeline using the Drive360 dataset and tensorflow."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "papermill": {
     "duration": 0.028132,
     "end_time": "2019-10-20T00:04:04.396314",
     "exception": false,
     "start_time": "2019-10-20T00:04:04.368182",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "exp = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "papermill": {
     "duration": 0.030814,
     "end_time": "2019-10-20T00:04:04.438393",
     "exception": false,
     "start_time": "2019-10-20T00:04:04.407579",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "exp.append(\n",
    "    {\n",
    "        'modelclass': BiGRU_DO,\n",
    "        'modelname': 'C_BiGRU_',\n",
    "        'num_lstm_layers': 3,\n",
    "        'hidden_lstm_size': 64,\n",
    "        'p_dropout': 0.6,\n",
    "        'epochs': 60,\n",
    "        'loss': 'both',     #either both, steer, speed\n",
    "        'bins': None,\n",
    "        'num_bins_min': 2000,\n",
    "        'sample_0': False,\n",
    "        'use_classes': False, #true or false\n",
    "        'lr': 0.003,\n",
    "        'lr_decay': [20, 30, 40],\n",
    "        'pickle_files': ['densenet121_aug', 'resnet34_high', 'densenet201_high']   \n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "papermill": {
     "duration": 0.035667,
     "end_time": "2019-10-20T00:04:04.485656",
     "exception": false,
     "start_time": "2019-10-20T00:04:04.449989",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "train_loader = None\n",
    "validation_loader = None\n",
    "test_loader = None\n",
    "pickle_files = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "papermill": {
     "duration": 0.062742,
     "end_time": "2019-10-20T00:04:04.560232",
     "exception": false,
     "start_time": "2019-10-20T00:04:04.497490",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "44"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import gc\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 408
    },
    "colab_type": "code",
    "id": "k_Pdj33i6OoL",
    "outputId": "9bc753ab-ae87-46c7-a023-de811df4f7b8",
    "papermill": {
     "duration": 3860.103072,
     "end_time": "2019-10-20T01:08:24.675456",
     "exception": false,
     "start_time": "2019-10-20T00:04:04.572384",
     "status": "completed"
    },
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Phase: train # of data: 156029\n",
      "Phase: validation # of data: 10266\n",
      "Phase: test # of data: 27920\n",
      "Loaded train loader with the following data available as a dict.\n",
      "Index(['cameraRight', 'cameraFront', 'cameraRear', 'cameraLeft', 'canSteering',\n",
      "       'canSpeed', 'chapter'],\n",
      "      dtype='object')\n",
      "start\n",
      "{'modelclass': <class 'learntodrive.models.New_BasicBiGRU_BatchNorm_BigTopLayer_DenseNet_Buckets.New_BasicBiGRUBN_BigDenseNet_Buckets_DO'>, 'modelname': 'C_BiGRU_', 'num_lstm_layers': 3, 'hidden_lstm_size': 64, 'p_dropout': 0.6, 'epochs': 60, 'loss': 'both', 'bins': None, 'num_bins_min': 2000, 'sample_0': False, 'use_classes': False, 'lr': 0.003, 'lr_decay': [20, 30, 40], 'pickle_files': ['densenet121_aug', 'resnet34_high', 'densenet201_high']}\n",
      "[epoch: 1, batch:     20] loss: 1.34074 time load: 0.15670 time model: 0.05113\n",
      "[epoch: 1, batch:     40] loss: 0.78753 time load: 0.09134 time model: 0.04754\n",
      "[epoch: 1, batch:     60] loss: 0.59022 time load: 0.07860 time model: 0.04733\n",
      "[epoch: 1, batch:     80] loss: 0.54337 time load: 0.08342 time model: 0.04803\n",
      "[epoch: 1, batch:    100] loss: 0.46552 time load: 0.07906 time model: 0.04854\n",
      "[epoch: 1, batch:    120] loss: 0.42159 time load: 0.07759 time model: 0.04743\n",
      "[epoch: 1, batch:    140] loss: 0.42893 time load: 0.08372 time model: 0.04753\n",
      "[epoch: 1, batch:    160] loss: 0.46443 time load: 0.08165 time model: 0.04459\n",
      "[epoch: 1, batch:    180] loss: 0.40391 time load: 0.08340 time model: 0.04595\n",
      "[epoch: 1, batch:    200] loss: 0.39854 time load: 0.07747 time model: 0.04659\n",
      "[epoch: 1, batch:    220] loss: 0.45580 time load: 0.08413 time model: 0.04587\n",
      "[epoch: 1, batch:    240] loss: 0.42271 time load: 0.08057 time model: 0.04655\n",
      "[epoch: 1, batch:    260] loss: 0.37373 time load: 0.08821 time model: 0.04640\n",
      "[epoch: 1, batch:    280] loss: 0.37218 time load: 0.07992 time model: 0.04575\n",
      "[epoch: 1, batch:    300] loss: 0.42728 time load: 0.08061 time model: 0.04564\n",
      "[epoch: 1, batch:    320] loss: 0.36589 time load: 0.07485 time model: 0.04811\n",
      "[epoch: 1, batch:    340] loss: 0.39677 time load: 0.08091 time model: 0.04713\n",
      "[epoch: 1, batch:    360] loss: 0.38864 time load: 0.07742 time model: 0.04616\n",
      "[epoch: 1, batch:    380] loss: 0.32931 time load: 0.08418 time model: 0.04476\n",
      "[epoch: 1, batch:    400] loss: 0.30850 time load: 0.08311 time model: 0.04416\n",
      "[epoch: 1, batch:    420] loss: 0.33858 time load: 0.08212 time model: 0.04420\n",
      "[epoch: 1, batch:    440] loss: 0.37356 time load: 0.08115 time model: 0.04324\n",
      "[epoch: 1, batch:    460] loss: 0.36877 time load: 0.07966 time model: 0.04535\n",
      "[epoch: 1, batch:    480] loss: 0.39913 time load: 0.08000 time model: 0.04373\n",
      "[epoch: 1, batch:    500] loss: 0.33858 time load: 0.07622 time model: 0.04618\n",
      "[epoch: 1, batch:    520] loss: 0.38923 time load: 0.07547 time model: 0.04516\n",
      "[epoch: 1, batch:    540] loss: 0.29207 time load: 0.07696 time model: 0.04515\n",
      "[epoch: 1, batch:    560] loss: 0.34649 time load: 0.07802 time model: 0.04574\n",
      "[epoch: 1, batch:    580] loss: 0.35517 time load: 0.07699 time model: 0.04438\n",
      "[epoch: 1, batch:    600] loss: 0.36542 time load: 0.07880 time model: 0.04446\n",
      "[epoch: 1, batch:    620] loss: 0.36461 time load: 0.08177 time model: 0.04552\n",
      "[epoch: 1, batch:    640] loss: 0.34902 time load: 0.07579 time model: 0.04407\n",
      "[epoch: 1, batch:    660] loss: 0.35477 time load: 0.08619 time model: 0.04499\n",
      "[epoch: 1, batch:    680] loss: 0.35909 time load: 0.08138 time model: 0.04480\n",
      "[epoch: 1, batch:    700] loss: 0.32287 time load: 0.07492 time model: 0.04434\n",
      "[epoch: 1, batch:    720] loss: 0.34504 time load: 0.07910 time model: 0.04626\n",
      "[epoch: 1, batch:    740] loss: 0.33082 time load: 0.08399 time model: 0.04482\n",
      "[epoch: 1, batch:    760] loss: 0.36727 time load: 0.09077 time model: 0.04527\n",
      "[epoch: 1, batch:    780] loss: 0.31094 time load: 0.07852 time model: 0.04395\n",
      "[epoch: 1, batch:    800] loss: 0.35350 time load: 0.07636 time model: 0.04549\n",
      "[epoch: 1, batch:    820] loss: 0.35999 time load: 0.07378 time model: 0.04391\n",
      "[epoch: 1, batch:    840] loss: 0.31657 time load: 0.07845 time model: 0.04532\n",
      "[epoch: 1, batch:    860] loss: 0.34578 time load: 0.07541 time model: 0.04569\n",
      "[epoch: 1, batch:    880] loss: 0.35178 time load: 0.07713 time model: 0.04468\n",
      "[epoch: 1, batch:    900] loss: 0.34938 time load: 0.07765 time model: 0.04635\n",
      "[epoch: 1, batch:    920] loss: 0.31727 time load: 0.07656 time model: 0.04568\n",
      "[epoch: 1, batch:    940] loss: 0.35040 time load: 0.08001 time model: 0.04630\n",
      "[epoch: 1, batch:    960] loss: 0.36803 time load: 0.08087 time model: 0.04500\n",
      "[epoch: 1, batch:    980] loss: 0.31821 time load: 0.08076 time model: 0.04503\n",
      "[epoch: 1, batch:   1000] loss: 0.31624 time load: 0.07901 time model: 0.04484\n",
      "[epoch: 1, batch:   1020] loss: 0.33570 time load: 0.07546 time model: 0.04544\n",
      "[epoch: 1, batch:   1040] loss: 0.32007 time load: 0.08317 time model: 0.04469\n",
      "[epoch: 1, batch:   1060] loss: 0.31366 time load: 0.07948 time model: 0.04504\n",
      "[epoch: 1, batch:   1080] loss: 0.31909 time load: 0.07697 time model: 0.04527\n",
      "[epoch: 1, batch:   1100] loss: 0.33220 time load: 0.07839 time model: 0.04517\n",
      "[epoch: 1, batch:   1120] loss: 0.32325 time load: 0.08199 time model: 0.04555\n"
     ]
    }
   ],
   "source": [
    "for exps in exp:\n",
    "    # create a train, validation and test data loader\n",
    "    train_loader = Drive360Loader(config, 'train', exps['bins'], exps['num_bins_min'], exps['sample_0'])\n",
    "    validation_loader = Drive360Loader(config, 'validation')\n",
    "    test_loader = Drive360Loader(config, 'test')\n",
    "\n",
    "    # print the data (keys) available for use. See full \n",
    "    # description of each data type in the documents.\n",
    "    print('Loaded train loader with the following data available as a dict.')\n",
    "    print(train_loader.drive360.dataframe.keys())\n",
    "    epochs = exps['epochs']\n",
    "    modelname = exps['modelname']\n",
    "    use_classes = exps['use_classes']\n",
    "    num_lstm_layers = exps['num_lstm_layers']\n",
    "    hidden_lstm_size = exps['hidden_lstm_size']\n",
    "    bins = exps['bins']\n",
    "    p_dropout = exps['p_dropout']\n",
    "    if bins==None:\n",
    "        num_classes = 1\n",
    "    else:\n",
    "        num_classes = len(bins)-1\n",
    "    # model path to save in google drive\n",
    "    MODEL_FINAL_PATH = 'models/'+ modelname + 'model_final.pth'\n",
    "    STEER_PATH = 'models/'+ modelname + 'model_steer.pth'\n",
    "    SPEED_PATH = 'models/'+ modelname + 'model_speed.pth'\n",
    "    LOGFILE_PATH = 'logs/' + modelname + 'logfile.log'\n",
    "    SUBMISSION_FILENAME = 'Submissions/'+ modelname + 'submission.csv'\n",
    "    SUBMISSION_FILENAME_FINAL = 'Submissions/'+ modelname + '_finalmodel_submission.csv'\n",
    "    VALIDATION_FILENAME = 'Validation/'+ modelname + 'validation.csv'\n",
    "    VAILDATION_FILENAME_FINAL = 'Validation/'+ modelname + '_finalmodel_validation.csv'\n",
    "\n",
    "    pickle_files = [loadPickle(dict_pickle[x]) for x in exps['pickle_files']]\n",
    "    num_hidden_features = 0\n",
    "    for x in pickle_files:\n",
    "        num_hidden_features += x[list(x.keys())[0]].shape[0]\n",
    "\n",
    "    modelclass = exps['modelclass']\n",
    "    model = modelclass(num_cnn_features=num_hidden_features, \n",
    "                       num_lstm_layers=num_lstm_layers, \n",
    "                       hidden_lstm_size=hidden_lstm_size, \n",
    "                       num_classes=num_classes,\n",
    "                       p_dropout=p_dropout).cuda()\n",
    "    criterion_speed = nn.MSELoss()\n",
    "    if exps['use_classes']:\n",
    "        criterion_angle = nn.CrossEntropyLoss()\n",
    "    else:\n",
    "        criterion_angle = nn.MSELoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=exps['lr'])\n",
    "    current_lr = exps['lr']\n",
    "    lr_decay_idx = 0\n",
    "    lrates = [0.003, 0.003, 0.001, 0.001]\n",
    "    lowestSpeedLoss = 999999\n",
    "    lowestSteerLoss = 999999\n",
    "    log_textfile(LOGFILE_PATH, 'start')\n",
    "    log_textfile(LOGFILE_PATH, str(exps))\n",
    "    for epoch in range(epochs):\n",
    "        total_loss = 0.0\n",
    "        running_loss = 0.0\n",
    "        running_model = 0.0\n",
    "        running_load = 0.0\n",
    "        end = time.time()\n",
    "        model.train()\n",
    "        for batch_idx, (data, target, front_name) in enumerate(train_loader):\n",
    "            data = move_data_to_cuda(data)\n",
    "            target = move_target_to_cuda(target)\n",
    "            hidden_features = [get_features_3(front_name, x).cuda() for x in pickle_files]\n",
    "            start = time.time()\n",
    "            delta_load = start-end\n",
    "            running_load += delta_load\n",
    "            optimizer.zero_grad()\n",
    "            prediction = model(data, hidden_features)\n",
    "            loss_speed = criterion_speed(prediction['canSpeed'], target['canSpeed'])\n",
    "            if use_classes:\n",
    "                target['canSteering'] = target['canSteering'].long().cuda()\n",
    "            loss_steering = criterion_angle(prediction['canSteering'], target['canSteering'])\n",
    "            if exps['loss'] == 'steer':\n",
    "                loss = loss_steering\n",
    "            if exps['loss'] == 'speed':\n",
    "                loss = loss_speed\n",
    "            if exps['loss'] == 'both':\n",
    "                loss = loss_speed+loss_steering\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            # print statistics\n",
    "            running_loss += loss.item()\n",
    "            total_loss += loss.item()\n",
    "            end = time.time()\n",
    "            delta_model = end - start\n",
    "            running_model += delta_model\n",
    "            if (batch_idx+1) % 20 == 0:  \n",
    "                log_textfile(LOGFILE_PATH, '[epoch: %d, batch:  %5d] loss: %.5f time load: %.5f time model: %.5f' % (epoch + 1, batch_idx + 1, running_loss / 20.0, running_load / 20.0, running_model / 20.0))\n",
    "                running_loss = 0.0\n",
    "                running_model = 0.0\n",
    "                running_load = 0.0\n",
    "        if epoch+1 in exps['lr_decay']:\n",
    "            current_lr = current_lr / 2\n",
    "            for param_group in optimizer.param_groups:\n",
    "                param_group[\"lr\"] = current_lr\n",
    "            log_textfile(LOGFILE_PATH, 'LR:' + str(current_lr))\n",
    "            lr_decay_idx += 1\n",
    "        shuffle(train_loader.drive360.indices)\n",
    "        if use_classes:\n",
    "            mse_steer, mse_speed = run_validation(model, validation_loader, config, pickle_files, train_loader=train_loader)\n",
    "        else:\n",
    "            mse_steer, mse_speed = run_validation(model, validation_loader, config, pickle_files, train_loader=None)\n",
    "        torch.save(model, MODEL_FINAL_PATH)\n",
    "        if mse_steer < lowestSteerLoss:\n",
    "            torch.save(model, STEER_PATH)\n",
    "            lowestSteerLoss = mse_steer\n",
    "        if mse_speed < lowestSpeedLoss:\n",
    "            torch.save(model, SPEED_PATH)\n",
    "            lowestSpeedLoss = mse_speed\n",
    "        log_textfile(LOGFILE_PATH, \"Epoch: \" + str(epoch + 1) + \"/\" + str(epochs) + \" Tr loss: \" + str(round(total_loss/batch_idx,3)) + \" MSESteer: \" + str(round(mse_steer,3)) + \" MSESpeed: \" + str(round(mse_speed,3)) + \" Lowest MSESteer: \" + str(round(lowestSteerLoss,3)) + \" Lowest MSESpeed: \" + str(round(lowestSpeedLoss,3)))\n",
    "    print('Best Steer Loss:', lowestSteerLoss)\n",
    "    print('Best Speed Loss:', lowestSpeedLoss)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "papermill": {
     "duration": 217.417135,
     "end_time": "2019-10-20T01:12:02.543239",
     "exception": false,
     "start_time": "2019-10-20T01:08:25.126104",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Submission file has no NAs!\n",
      "Submission file has no NAs!\n"
     ]
    }
   ],
   "source": [
    "modelSteer = torch.load(STEER_PATH)\n",
    "modelSpeed = torch.load(SPEED_PATH)\n",
    "if use_classes:\n",
    "    df = predict_submission(modelSteer, modelSpeed, pickle_files, test_loader, config, train_loader=train_loader)\n",
    "else:\n",
    "    df = predict_submission(modelSteer, modelSpeed, pickle_files, test_loader, config, train_loader=None)\n",
    "create_submission(df, '../Data/test_full.csv', SUBMISSION_FILENAME)\n",
    "\n",
    "modelSteer = torch.load(MODEL_FINAL_PATH)\n",
    "modelSpeed = torch.load(MODEL_FINAL_PATH)\n",
    "if use_classes:\n",
    "    df = predict_submission(modelSteer, modelSpeed, pickle_files, test_loader, config, train_loader=train_loader)\n",
    "else:\n",
    "    df = predict_submission(modelSteer, modelSpeed, pickle_files, test_loader, config, train_loader=None)\n",
    "create_submission(df, '../Data/test_full.csv', SUBMISSION_FILENAME_FINAL)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "Model Simple 2 Sample3_subset.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  },
  "papermill": {
   "duration": 4364.38263,
   "end_time": "2019-10-20T01:12:09.014028",
   "environment_variables": {},
   "exception": null,
   "input_path": "Long_run_3.ipynb",
   "output_path": "long_out_3.ipynb",
   "parameters": {},
   "start_time": "2019-10-19T23:59:24.631398",
   "version": "1.2.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
